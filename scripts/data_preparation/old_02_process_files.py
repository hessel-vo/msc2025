import logging
from pathlib import Path
import time
import os
from dotenv import load_dotenv
from typing import Iterator, Dict, List, Tuple
import json
import sys

from pii_redaction import redact_pii, ALL_PATTERNS_RECOMMENDED

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Quality Heuristic Constants ---
# ... (These constants remain unchanged)
MAX_AVG_LINE_LENGTH = 100
MAX_LINE_LENGTH = 1000
MIN_ALPHANUM_RATIO = 0.25
NUM_LINES_TO_CHECK_FOR_GENERATED = 10
GENERATED_KEYWORDS = {
    "auto-generated", "automatically generated", "do not edit",
    "autogenerated", "this file was generated", "generated file",
}

# --- Boilerplate Removal Constants ---
MAX_HEADER_SCAN_LINES = 10
BOILERPLATE_KEYWORDS = {
    "copyright", "license", "licensed", "author", "authors",
    "all rights reserved", "gpl", "lgpl", "mit license", "apache license",
    "bsd license",
}

# The comprehensive COMMENT_MARKERS dictionary remains the same
COMMENT_MARKERS = {
    # --- By Filename (for specific files without standard extensions) ---
    "CMakeLists.txt": {"#"},

    # --- By Extension ---
    # Hash-based Comments (#)
    ".py": {"#"},
    ".sh": {"#"},
    ".conf": {"#"},
    ".toml": {"#"},
    ".yaml": {"#"},
    ".yml": {"#"},
    ".vspec": {"#"},
    ".doxyfile": {"#"},
    ".rc": {"#"},

    # Batch File Comments
    ".bat": {"::"},

    # Lua Comments
    ".lua": {"--", ("--[[", "]]")},

    # PlantUML Comments
    ".puml": {"'", ("/'", "'/")},

    # Jinja2 Template Comments
    ".jinja2": {("{#", "#}")},
    ".j2": {("{#", "#}")},

    # C-style Comments (line and/or block)
    ".c": {"//", ("/*", "*/")},
    ".cpp": {"//", ("/*", "*/")},
    ".h": {"//", ("/*", "*/")},
    ".hpp": {"//", ("/*", "*/")},
    ".java": {"//", ("/*", "*/")},
    ".js": {"//", ("/*", "*/")},
    ".ts": {"//", ("/*", "*/")},
    ".go": {"//", ("/*", "*/")},
    ".rs": {"//", ("/*", "*/")},
    ".kt": {"//", ("/*", "*/")},
    ".kts": {"//", ("/*", "*/")},
    ".swift": {"//", ("/*", "*/")},
    ".m": {"//", ("/*", "*/")},
    ".mm": {"//", ("/*", "*/")},
    ".franca": {"//", ("/*", "*/")},
    ".dtsi": {"//", ("/*", "*/")},
    ".overlay": {"//", ("/*", "*/")},
    ".gradle": {"//", ("/*", "*/")},

    # C-style Comments (line only)
    ".proto": {"//"},
    ".fbs": {"//"},
    ".fidl": {"//"},
    
    ".css": {("/*", "*/")},

    # XML-style Comments (block only)
    ".html": {("<!--", "-->")},
    ".xml": {("<!--", "-->")},
    ".ui": {("<!--", "-->")},
    ".arxml": {("<!--", "-->")},
    ".dox": {("<!--", "-->")},
}

def load_documents_from_manifest(manifest_path: Path, repos_root: Path, failed_decodes: List[str]) -> Iterator[Dict[str, str]]:
    # This function remains unchanged
    # ...
    logging.info(f"Reading file paths from manifest: '{manifest_path}'")
    with open(manifest_path, 'r', encoding='utf-8') as f:
        for line in f:
            full_path = Path(line.strip())
            
            if not full_path.is_file():
                logging.warning(f"File not found, skipping: {full_path}")
                continue

            try:
                relative_path = full_path.relative_to(repos_root)
                repo_id = relative_path.parts[0]
                path_in_repo = str(Path(*relative_path.parts[1:]))
                content = full_path.read_text(encoding='utf-8')
                
                yield {
                    'repo_id': repo_id,
                    'path_in_repo': path_in_repo,
                    'content': content
                }

            except UnicodeDecodeError:
                logging.warning(f"Could not decode file as UTF-8, skipping: {full_path}")
                failed_decodes.append(str(full_path))
                continue
            except (ValueError, IndexError) as e:
                logging.error(f"Error processing path\n '{full_path}'. \nIs it correctly located under\n '{repos_root}'? Error: {e}")
                continue


def passes_quality_heuristics(doc: Dict[str, str]) -> Tuple[bool, str]:
    # This function remains unchanged
    # ...
    content = doc['content']
    path_in_repo = doc['path_in_repo']
    lines = content.splitlines()
    
    is_markdown = path_in_repo.endswith(('.md', '.rst', '.adoc'))

    if not is_markdown:
        header_text = "\n".join(lines[:NUM_LINES_TO_CHECK_FOR_GENERATED]).lower()
        for keyword in GENERATED_KEYWORDS:
            if keyword in header_text:
                return False, "generated"

    if not content.strip():
        return False, "is_empty"

    if not is_markdown:
        if lines and max(len(line) for line in lines) > MAX_LINE_LENGTH:
            return False, "max_line_length"
        if lines and len(content) / len(lines) > MAX_AVG_LINE_LENGTH:
            return False, "avg_line_length"
        
    if len(content) > 0 and (sum(1 for char in content if char.isalnum()) / len(content)) < MIN_ALPHANUM_RATIO:
        return False, "alphanum_ratio"
        
    return True, ""


def _find_comment_block(lines: List[str], start_index: int, line_marker: str, block_markers: tuple) -> Tuple[int, str]:
    # This helper function remains unchanged
    # ...
    stripped_line = lines[start_index].strip()

    if block_markers and stripped_line.startswith(block_markers[0]):
        block_lines = []
        for i in range(start_index, len(lines)):
            line = lines[i]
            block_lines.append(line)
            if block_markers[1] in line:
                return i, "\n".join(block_lines)
        return len(lines) - 1, "\n".join(block_lines)

    if line_marker and stripped_line.startswith(line_marker):
        block_lines = []
        for i in range(start_index, len(lines)):
            line = lines[i].strip()
            if not line:
                return i - 1, "\n".join(block_lines)
            if line.startswith(line_marker):
                block_lines.append(lines[i])
            else:
                return i - 1, "\n".join(block_lines)
        return len(lines) - 1, "\n".join(block_lines)
    
    return None, None


# REWRITTEN: Final version that correctly handles shebang preservation
def remove_boilerplate(doc: Dict[str, str]) -> Dict[str, str]:
    """
    Finds the first boilerplate comment block within the header scan limit and removes it,
    correctly preserving a shebang if present.
    """
    path = Path(doc['path_in_repo'])
    markers = COMMENT_MARKERS.get(path.suffix.lower())
    
    if not markers:
        markers = COMMENT_MARKERS.get(path.name)
    
    if not markers:
        return doc

    line_marker = None
    block_markers = None
    for m in markers:
        if isinstance(m, str):
            line_marker = m
        elif isinstance(m, tuple):
            block_markers = m

    lines = doc['content'].splitlines()
    if not lines:
        return doc
    
    # --- MODIFIED: Re-introducing the robust shebang handling ---
    has_shebang = lines[0].strip().startswith("#!")
    start_scan_index = 1 if has_shebang else 0
    
    block_to_remove = None
    scan_limit = min(len(lines), MAX_HEADER_SCAN_LINES)

    # --- Step 1: Search for the first boilerplate block ---
    # The loop now correctly starts AFTER the shebang if it exists
    for i in range(start_scan_index, scan_limit):
        if not lines[i].strip():
            continue

        block_end_index, block_text = _find_comment_block(lines, i, line_marker, block_markers)
        
        if block_end_index is None:
            continue
            
        is_boilerplate = any(keyword in block_text.lower() for keyword in BOILERPLATE_KEYWORDS)

        if is_boilerplate:
            block_to_remove = (i, block_end_index)
            break
            
    # --- Step 2: Reconstruct the content ---
    if block_to_remove:
        start_index, end_index = block_to_remove
        
        # MODIFIED: The logic to reconstruct the list is now simpler and correct
        # It takes everything before the block, and everything after.
        # The shebang is automatically preserved because it's part of `lines[:start_index]`
        final_lines = lines[:start_index] + lines[end_index + 1:]
        doc['content'] = "\n".join(final_lines)
        
    return doc


def main():
    # ... (The main function remains entirely unchanged)
    start_time = time.time()
    
    load_dotenv()
    project_root_str = os.getenv('PROJECT_ROOT')
    if not project_root_str:
        logging.error("'PROJECT_ROOT' environment variable not set. Please check your .env file.")
        return
        
    project_root = Path(project_root_str)
    data_prep_dir = project_root / "scripts" / "data_preparation"
    repos_root = project_root / "repositories" / "all_repos"

    if len(sys.argv) > 1:
        filter_level = sys.argv[1]
        manifest_path = data_prep_dir / "01_filtering" / f"filtered_file_paths_{filter_level}.txt"
        logging.info(f"Using manifest file suffix from command line: '{filter_level}'")
    else:
        logging.info("No command-line suffix provided. Provide a manifest file suffix.")
        return
    
    processing_dir = data_prep_dir / "02_processing" / filter_level
    processing_dir.mkdir(parents=True, exist_ok=True)

    failed_decodes_path = processing_dir / f"failed_file_decodes_{filter_level}.txt"
    processed_data_path = processing_dir / f"processed_data_{filter_level}.jsonl"

    if not manifest_path.is_file():
        logging.error(f"Manifest file not found: '{manifest_path}'")
        return

    failed_decodes = []
    total_docs_loaded = 0
    passed_docs_count = 0
    
    rejection_reasons = {
        "generated": [], "is_empty": [], "max_line_length": [],
        "avg_line_length": [], "alphanum_ratio": [], "empty_after_processing": [],
    }

    logging.info(f"Starting data loading, quality filtering, and boilerplate removal...")

    try:
        document_generator = load_documents_from_manifest(manifest_path, repos_root, failed_decodes)
        
        with open(processed_data_path, 'w', encoding='utf-8') as f_out:
            for doc in document_generator:
                total_docs_loaded += 1
                passes, reason = passes_quality_heuristics(doc)

                if passes:
                    processed_doc = remove_boilerplate(doc)
                    processed_doc['content'] = redact_pii(processed_doc['content'], ALL_PATTERNS_RECOMMENDED)

                    if processed_doc['content'].strip():
                        f_out.write(json.dumps(processed_doc) + '\n')
                        passed_docs_count += 1
                    else:
                        reason = "empty_after_processing"
                        repo_and_path = f"{doc['repo_id']}/{doc['path_in_repo']}"
                        rejection_reasons[reason].append(repo_and_path)
                else:
                    repo_and_path = f"{doc['repo_id']}/{doc['path_in_repo']}"
                    rejection_reasons[reason].append(repo_and_path)
    except Exception as e:
        logging.error(f"An unexpected error occurred during document processing: {e}", exc_info=True)
        return

    logging.info("--- Processing Summary ---")
    logging.info(f"Total documents loaded: {total_docs_loaded:,}")
    logging.info(f"Documents passed processing: {passed_docs_count:,}")
    logging.info(f"Documents rejected: {total_docs_loaded - passed_docs_count:,}")
    logging.info("Rejection reasons:")
    for reason, paths in rejection_reasons.items():
        logging.info(f"  - {reason}: {len(paths):,}")
    
    if failed_decodes:
        logging.warning(f"Found {len(failed_decodes)} files that failed UTF-8 decoding.")
        logging.info(f"Writing list of failed files to: {failed_decodes_path}")
        with open(failed_decodes_path, 'w', encoding='utf-8') as f_out:
            for path in failed_decodes:
                f_out.write(f"{path}\n")

    logging.info("Writing rejection logs...")
    all_rejected_paths = []
    for reason, paths in rejection_reasons.items():
        if paths:
            reason_log_path = processing_dir / f"rejected_files_{filter_level}_{reason}.txt"
            with open(reason_log_path, 'w', encoding='utf-8') as f_reason:
                for path_str in paths:
                    f_reason.write(f"{path_str}\n")
            all_rejected_paths.extend(paths)
    
    if all_rejected_paths:
        all_rejected_log_path = processing_dir / f"all_rejected_files_{filter_level}.txt"
        with open(all_rejected_log_path, 'w', encoding='utf-8') as f_all:
            for path_str in sorted(all_rejected_paths):
                f_all.write(f"{path_str}\n")
    
    logging.info("Rejection logs written successfully.")
    end_time = time.time()
    logging.info(f"Script execution finished in {end_time - start_time:.2f} seconds.")


if __name__ == "__main__":
    main()