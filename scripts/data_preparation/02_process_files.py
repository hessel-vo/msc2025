import logging
from pathlib import Path
import time
import os
from dotenv import load_dotenv
from typing import Iterator, Dict, List, Tuple
import json

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Quality Heuristic Constants ---
MAX_AVG_LINE_LENGTH = 100
MAX_LINE_LENGTH = 1000
MIN_ALPHANUM_RATIO = 0.25

NUM_LINES_TO_CHECK_FOR_GENERATED = 10
GENERATED_KEYWORDS = {
    "auto-generated",
    "automatically generated",
    "do not edit",
    "autogenerated",
    "this file was generated",
    "generated file",
}

def load_documents_from_manifest(manifest_path: Path, repos_root: Path, failed_decodes: List[str]) -> Iterator[Dict[str, str]]:
    # This function remains the same as before
    logging.info(f"Reading file paths from manifest: '{manifest_path}'")
    # ... (rest of the function is unchanged)
    with open(manifest_path, 'r', encoding='utf-8') as f:
        for line in f:
            full_path = Path(line.strip())
            
            if not full_path.is_file():
                logging.warning(f"File not found, skipping: {full_path}")
                continue

            try:
                relative_path = full_path.relative_to(repos_root)
                repo_id = relative_path.parts[0]
                path_in_repo = str(Path(*relative_path.parts[1:]))
                content = full_path.read_text(encoding='utf-8')
                
                yield {
                    'repo_id': repo_id,
                    'path_in_repo': path_in_repo,
                    'content': content
                }

            except UnicodeDecodeError:
                logging.warning(f"Could not decode file as UTF-8, skipping: {full_path}")
                failed_decodes.append(str(full_path))
                continue
            except (ValueError, IndexError) as e:
                logging.error(f"Error processing path\n '{full_path}'. \nIs it correctly located under\n '{repos_root}'? Error: {e}")
                continue

def passes_quality_heuristics(doc: Dict[str, str]) -> Tuple[bool, str]:
    """
    Applies a series of quality heuristics to a document.

    Args:
        doc: The document dictionary containing content and path_in_repo.

    Returns:
        A tuple of (bool, str), where the bool is True if the document
        passes all checks, and the string contains the reason for failure.
    """
    content = doc['content']
    path_in_repo = doc['path_in_repo']
    lines = content.splitlines()
    
    # Check if the file is markdown once at the beginning
    is_markdown = path_in_repo.endswith('.md')

    # 1. Check for auto-generation keywords (skip for .md files)
    if not is_markdown:
        header_text = "\n".join(lines[:NUM_LINES_TO_CHECK_FOR_GENERATED]).lower()
        for keyword in GENERATED_KEYWORDS:
            if keyword in header_text:
                return False, "generated"

    # 2. Handle empty files
    if not content.strip():
        return False, "is_empty"

    # 3. Check line length heuristics (skip for .md files)
    if not is_markdown:
        if lines and max(len(line) for line in lines) > MAX_LINE_LENGTH:
            return False, "max_line_length"

        if lines and len(content) / len(lines) > MAX_AVG_LINE_LENGTH:
            return False, "avg_line_length"
        
    # 4. Check for alphanumeric character ratio (applies to all files)
    if len(content) > 0 and (sum(1 for char in content if char.isalnum()) / len(content)) < MIN_ALPHANUM_RATIO:
        return False, "alphanum_ratio"
        
    return True, ""

def main():
    """Main execution function."""
    start_time = time.time()
    
    # --- Path Setup ---
    load_dotenv()
    project_root_str = os.getenv('PROJECT_ROOT')
    if not project_root_str:
        logging.error("'PROJECT_ROOT' environment variable not set. Please check your .env file.")
        return
        
    project_root = Path(project_root_str)
    data_prep_dir = project_root / "scripts" / "data_preparation"
    repos_root = project_root / "repositories" / "all_repos"
    manifest_path = data_prep_dir / "01_filtering" / "filtered_file_paths.txt"
    
    processing_dir = data_prep_dir / "02_processing"
    processing_dir.mkdir(parents=True, exist_ok=True)

    failed_decodes_path = processing_dir / "failed_file_decodes.txt"
    quality_filtered_path = processing_dir / "quality_filtered_data.jsonl"

    if not manifest_path.is_file():
        logging.error(f"Manifest file not found: '{manifest_path}'")
        return

    # --- Data Loading and Processing ---
    failed_decodes = []
    total_docs_loaded = 0
    passed_docs_count = 0
    
    rejection_reasons = {
        "generated": [],
        "is_empty": [],
        "max_line_length": [],
        "avg_line_length": [],
        "alphanum_ratio": [],
    }

    logging.info(f"Starting data loading and quality filtering...")

    try:
        document_generator = load_documents_from_manifest(manifest_path, repos_root, failed_decodes)
        
        with open(quality_filtered_path, 'w', encoding='utf-8') as f_out:
            for doc in document_generator:
                total_docs_loaded += 1
                passes, reason = passes_quality_heuristics(doc)

                if passes:
                    f_out.write(json.dumps(doc) + '\n')
                    passed_docs_count += 1
                else:
                    if reason in rejection_reasons:
                        repo_and_path = f"{doc['repo_id']}/{doc['path_in_repo']}"
                        rejection_reasons[reason].append(repo_and_path)
                    
                    logging.debug(f"File failed quality check '{reason}': {doc['path_in_repo']}")

    except Exception as e:
        logging.error(f"An unexpected error occurred during document processing: {e}", exc_info=True)
        return

    # --- Final Logging and Artifact Generation ---
    # ... (This section remains unchanged)
    logging.info("--- Processing Summary ---")
    logging.info(f"Total documents loaded: {total_docs_loaded:,}")
    logging.info(f"Documents passed quality filters: {passed_docs_count:,}")
    logging.info(f"Documents rejected: {total_docs_loaded - passed_docs_count:,}")
    logging.info("Rejection reasons:")
    for reason, paths in rejection_reasons.items():
        logging.info(f"  - {reason}: {len(paths):,}")
    
    if failed_decodes:
        logging.warning(f"Found {len(failed_decodes)} files that failed UTF-8 decoding.")
        logging.info(f"Writing list of failed files to: {failed_decodes_path}")
        with open(failed_decodes_path, 'w', encoding='utf-8') as f_out:
            for path in failed_decodes:
                f_out.write(f"{path}\n")

    logging.info("Writing rejection logs...")
    all_rejected_paths = []
    for reason, paths in rejection_reasons.items():
        if paths:
            reason_log_path = processing_dir / f"rejected_files_{reason}.txt"
            with open(reason_log_path, 'w', encoding='utf-8') as f_reason:
                for path_str in paths:
                    f_reason.write(f"{path_str}\n")
            all_rejected_paths.extend(paths)
    
    if all_rejected_paths:
        all_rejected_log_path = processing_dir / "all_rejected_files.txt"
        with open(all_rejected_log_path, 'w', encoding='utf-8') as f_all:
            for path_str in sorted(all_rejected_paths):
                f_all.write(f"{path_str}\n")
    
    logging.info("Rejection logs written successfully.")

    end_time = time.time()
    logging.info(f"Script execution finished in {end_time - start_time:.2f} seconds.")


if __name__ == "__main__":
    main()