import logging
from pathlib import Path
import time
import os
from dotenv import load_dotenv
from typing import Iterator, Dict, List, Tuple
import json
import sys
import csv
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed

from tqdm import tqdm

# MODIFIED: Renamed import to reflect the new structure in pii_redaction.py
from pii_redaction import redact_pii, ALL_PATTERNS

# --- Configuration & Helper Functions ---

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Quality Heuristic Constants ---
# NEW: Added a constant for the maximum file size in bytes (1MB)
MAX_FILE_SIZE_BYTES = 450000
MAX_AVG_LINE_LENGTH = 100
MAX_LINE_LENGTH = 1000
MIN_ALPHANUM_RATIO = 0.25
NUM_LINES_TO_CHECK_FOR_GENERATED = 40
GENERATED_KEYWORDS = {
    "auto-generated", "automatically generated", "do not edit",
    "autogenerated", "this file was generated", "generated file", "is generated from"
}

# --- Boilerplate Removal Constants ---
MAX_HEADER_SCAN_LINES = 10
BOILERPLATE_KEYWORDS = {
    "copyright", "license", "licensed", "author", "authors",
    "all rights reserved", "gpl", "lgpl", "mit license", "apache license",
    "bsd license",
}

# The comprehensive COMMENT_MARKERS dictionary remains the same
COMMENT_MARKERS = {
    "CMakeLists.txt": {"#"}, ".py": {"#"}, ".sh": {"#"}, ".conf": {"#"},
    ".toml": {"#"}, ".yaml": {"#"}, ".yml": {"#"}, ".vspec": {"#"},
    ".doxyfile": {"#"}, ".rc": {"#"}, ".bat": {"::"}, ".lua": {"--", ("--[[", "]]")},
    ".puml": {"'", ("/'", "'/")}, ".jinja2": {("{#", "#}")}, ".j2": {("{#", "#}")},
    ".c": {"//", ("/*", "*/")}, ".cpp": {"//", ("/*", "*/")}, ".h": {"//", ("/*", "*/")},
    ".hpp": {"//", ("/*", "*/")}, ".java": {"//", ("/*", "*/")}, ".js": {"//", ("/*", "*/")},
    ".ts": {"//", ("/*", "*/")}, ".go": {"//", ("/*", "*/")}, ".rs": {"//", ("/*", "*/")},
    ".kt": {"//", ("/*", "*/")}, ".kts": {"//", ("/*", "*/")}, ".swift": {"//", ("/*", "*/")},
    ".m": {"//", ("/*", "*/")}, ".mm": {"//", ("/*", "*/")}, ".franca": {"//", ("/*", "*/")},
    ".dtsi": {"//", ("/*", "*/")}, ".overlay": {"//", ("/*", "*/")}, ".gradle": {"//", ("/*", "*/")},
    ".proto": {"//"}, ".fbs": {"//"}, ".fidl": {"//"}, ".css": {("/*", "*/")},
    ".html": {("<!--", "-->")}, ".xml": {("<!--", "-->")}, ".ui": {("<!--", "-->")},
    ".arxml": {("<!--", "-->")}, ".dox": {("<!--", "-->")},
}


def load_documents_from_manifest(manifest_path: Path, repos_root: Path, failed_decodes: List[str]) -> Iterator[Dict[str, str]]:
    # This function remains unchanged
    logging.info(f"Reading file paths from manifest: '{manifest_path}'")
    with open(manifest_path, 'r', encoding='utf-8') as f:
        for line in f:
            full_path = Path(line.strip())
            
            if not full_path.is_file():
                logging.warning(f"File not found, skipping: {full_path}")
                continue

            try:
                relative_path = full_path.relative_to(repos_root)
                repo_id = relative_path.parts[0]
                path_in_repo = str(Path(*relative_path.parts[1:]))
                content = full_path.read_text(encoding='utf-8')
                
                yield {
                    'repo_id': repo_id,
                    'path_in_repo': path_in_repo,
                    'content': content
                }

            except UnicodeDecodeError:
                logging.warning(f"Could not decode file as UTF-8, skipping: {full_path}")
                failed_decodes.append(str(full_path))
                continue
            except (ValueError, IndexError) as e:
                logging.error(f"Error processing path\n '{full_path}'. \nIs it correctly located under\n '{repos_root}'? Error: {e}")
                continue


# MODIFIED: Added the check for file size
def passes_quality_heuristics(doc: Dict[str, str]) -> Tuple[bool, str]:
    content = doc['content']
    path_in_repo = doc['path_in_repo']
    lines = content.splitlines()
    
    # Check for file size first, as it's a very cheap and effective filter
    if len(content) > MAX_FILE_SIZE_BYTES:
        return False, "file_too_large"

    is_markdown = path_in_repo.endswith(('.md', '.rst', '.adoc'))

    if not is_markdown:
        header_text = "\n".join(lines[:NUM_LINES_TO_CHECK_FOR_GENERATED]).lower()
        for keyword in GENERATED_KEYWORDS:
            if keyword in header_text:
                return False, "generated"

    if not content.strip():
        return False, "is_empty"

    if not is_markdown:
        if lines and max(len(line) for line in lines) > MAX_LINE_LENGTH:
            return False, "max_line_length"
        if lines and len(content) / len(lines) > MAX_AVG_LINE_LENGTH:
            return False, "avg_line_length"
        
    if len(content) > 0 and (sum(1 for char in content if char.isalnum()) / len(content)) < MIN_ALPHANUM_RATIO:
        return False, "alphanum_ratio"
        
    return True, ""


def _find_comment_block(lines: List[str], start_index: int, line_marker: str, block_markers: tuple) -> Tuple[int, str]:
    # This helper function remains unchanged
    stripped_line = lines[start_index].strip()

    if block_markers and stripped_line.startswith(block_markers[0]):
        block_lines = []
        for i in range(start_index, len(lines)):
            line = lines[i]
            block_lines.append(line)
            if block_markers[1] in line:
                return i, "\n".join(block_lines)
        return len(lines) - 1, "\n".join(block_lines)

    if line_marker and stripped_line.startswith(line_marker):
        block_lines = []
        for i in range(start_index, len(lines)):
            line = lines[i].strip()
            if not line:
                return i - 1, "\n".join(block_lines)
            if line.startswith(line_marker):
                block_lines.append(lines[i])
            else:
                return i - 1, "\n".join(block_lines)
        return len(lines) - 1, "\n".join(block_lines)
    
    return None, None


def remove_boilerplate(doc: Dict[str, str]) -> Dict[str, str]:
    # This function remains unchanged
    path = Path(doc['path_in_repo'])
    markers = COMMENT_MARKERS.get(path.suffix.lower())
    
    if not markers:
        markers = COMMENT_MARKERS.get(path.name)
    
    if not markers:
        return doc

    line_marker = None
    block_markers = None
    for m in markers:
        if isinstance(m, str):
            line_marker = m
        elif isinstance(m, tuple):
            block_markers = m

    lines = doc['content'].splitlines()
    if not lines:
        return doc
    
    has_shebang = lines[0].strip().startswith("#!")
    start_scan_index = 1 if has_shebang else 0
    
    block_to_remove = None
    scan_limit = min(len(lines), MAX_HEADER_SCAN_LINES)

    for i in range(start_scan_index, scan_limit):
        if not lines[i].strip():
            continue

        block_end_index, block_text = _find_comment_block(lines, i, line_marker, block_markers)
        
        if block_end_index is None:
            continue
            
        is_boilerplate = any(keyword in block_text.lower() for keyword in BOILERPLATE_KEYWORDS)

        if is_boilerplate:
            block_to_remove = (i, block_end_index)
            break
            
    if block_to_remove:
        start_index, end_index = block_to_remove
        final_lines = lines[:start_index] + lines[end_index + 1:]
        doc['content'] = "\n".join(final_lines)
        
    return doc


def process_document(doc: Dict[str, str]) -> Dict:
    # This worker function remains unchanged
    repo_and_path = f"{doc['repo_id']}/{doc['path_in_repo']}"
    
    passes, reason = passes_quality_heuristics(doc)
    if not passes:
        return {"status": "rejected", "reason": reason, "path": repo_and_path}
        
    doc = remove_boilerplate(doc)

    # In the provided code, the import was updated but not the function call. Correcting it here.
    redacted_text, found_pii = redact_pii(doc['content'], ALL_PATTERNS)
    doc['content'] = redacted_text

    if not doc['content'].strip():
        return {"status": "rejected", "reason": "empty_after_processing", "path": repo_and_path}
        
    return {"status": "passed", "doc": doc, "found_pii": found_pii, "path": repo_and_path}


def main():
    start_time = time.time()
    
    # --- Path Setup ---
    load_dotenv()
    # ... (rest of path setup is unchanged)
    project_root_str = os.getenv('PROJECT_ROOT')
    if not project_root_str:
        logging.error("'PROJECT_ROOT' environment variable not set. Please check your .env file.")
        return
        
    project_root = Path(project_root_str)
    data_prep_dir = project_root / "scripts" / "data_preparation"
    repos_root = project_root / "repositories" / "all_repos"

    if len(sys.argv) > 1:
        filter_level = sys.argv[1]
        manifest_path = data_prep_dir / "01_filtering" / f"filtered_file_paths_{filter_level}.txt"
        logging.info(f"Using manifest file suffix from command line: '{filter_level}'")
    else:
        logging.info("No command-line suffix provided. Provide a manifest file suffix.")
        return
    
    processing_dir = data_prep_dir / "02_processing" / filter_level
    processing_dir.mkdir(parents=True, exist_ok=True)

    failed_decodes_path = processing_dir / f"failed_file_decodes_{filter_level}.txt"
    processed_data_path = processing_dir / f"processed_data_{filter_level}.jsonl"
    
    if not manifest_path.is_file():
        logging.error(f"Manifest file not found: '{manifest_path}'")
        return

    # --- Data Loading ---
    failed_decodes = []
    # ... (rest of data loading is unchanged)
    logging.info("Loading all documents from manifest into memory...")
    document_generator = load_documents_from_manifest(manifest_path, repos_root, failed_decodes)
    docs_to_process = list(document_generator)
    total_docs_loaded = len(docs_to_process)
    logging.info(f"Loaded {total_docs_loaded:,} documents. Starting parallel processing...")

    # --- Parallel Processing ---
    passed_docs_count = 0
    
    # MODIFIED: Add the new rejection reason
    rejection_reasons = defaultdict(list)
    rejection_reasons_keys = [
        "file_too_large", "generated", "is_empty", "max_line_length",
        "avg_line_length", "alphanum_ratio", "empty_after_processing",
    ]
        
    pii_detections = {}

    with ProcessPoolExecutor(max_workers=8) as executor, open(processed_data_path, 'w', encoding='utf-8') as f_out:
        future_to_path = {
            executor.submit(process_document, doc): f"{doc['repo_id']}/{doc['path_in_repo']}" 
            for doc in docs_to_process
        }
        
        progress_bar = tqdm(as_completed(future_to_path), total=total_docs_loaded, desc="Processing files")
        
        for future in progress_bar:
            path = future_to_path[future]
            
            try:
                result = future.result()
                status = result["status"]

                if status == "passed":
                    processed_doc = result["doc"]
                    found_pii = result["found_pii"]
                    f_out.write(json.dumps(processed_doc) + '\n')
                    passed_docs_count += 1
                    if found_pii:
                        pii_detections[path] = found_pii
                else:
                    reason = result["reason"]
                    rejection_reasons[reason].append(path)
            except Exception as e:
                logging.error(f"A document failed during processing: {e}", exc_info=True)

    # --- Final Logging and Artifact Generation ---
    # The rest of the function remains unchanged.
    logging.info("--- Processing Summary ---")
    logging.info(f"Total documents loaded: {total_docs_loaded:,}")
    logging.info(f"Documents passed processing: {passed_docs_count:,}")
    logging.info(f"Documents rejected: {total_docs_loaded - passed_docs_count:,}")
    logging.info("Rejection reasons:")
    # MODIFIED: Iterate through the pre-defined keys to ensure consistent order
    for reason in rejection_reasons_keys:
        count = len(rejection_reasons[reason])
        logging.info(f"  - {reason}: {count:,}")
    
    # ... (rest of logging remains unchanged)
    logging.info("--- PII Detections Summary ---")
    pii_counts = defaultdict(int)
    for pii_list in pii_detections.values():
        for pii_type, _ in pii_list:
            pii_counts[pii_type] += 1
            
    if pii_counts:
        for pii_type, count in sorted(pii_counts.items()):
            logging.info(f"  - Found {pii_type}: {count:,} instances")
    else:
        logging.info("  - No PII detected.")

    if failed_decodes:
        logging.warning(f"Found {len(failed_decodes)} files that failed UTF-8 decoding.")
        logging.info(f"Writing list of failed files to: {failed_decodes_path}")
        with open(failed_decodes_path, 'w', encoding='utf-8') as f_out:
            for path in failed_decodes:
                f_out.write(f"{path}\n")

    logging.info("Writing rejection logs...")
    all_rejected_paths = []
    for reason, paths in rejection_reasons.items():
        if paths:
            reason_log_path = processing_dir / f"rejected_files_{filter_level}_{reason}.txt"
            with open(reason_log_path, 'w', encoding='utf-8') as f_reason:
                for path_str in paths:
                    f_reason.write(f"{path_str}\n")
            all_rejected_paths.extend(paths)
    
    if all_rejected_paths:
        all_rejected_log_path = processing_dir / f"all_rejected_files_{filter_level}.txt"
        with open(all_rejected_log_path, 'w', encoding='utf-8') as f_all:
            for path_str in sorted(all_rejected_paths):
                f_all.write(f"{path_str}\n")
    
    logging.info("Writing PII detection log...")
    if pii_detections:
        pii_log_path = processing_dir / f"pii_detections_log_{filter_level}.csv"
        with open(pii_log_path, 'w', newline='', encoding='utf-8') as f_pii:
            csv_writer = csv.writer(f_pii)
            csv_writer.writerow(['filepath', 'pii_type', 'matched_value'])
            
            for path_str, pii_instances in sorted(pii_detections.items()):
                for pii_type, matched_value in pii_instances:
                    csv_writer.writerow([path_str, pii_type, matched_value])

    logging.info("All logs written successfully.")
    end_time = time.time()
    logging.info(f"Script execution finished in {end_time - start_time:.2f} seconds.")


if __name__ == "__main__":
    main()